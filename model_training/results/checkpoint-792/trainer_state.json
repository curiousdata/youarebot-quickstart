{
  "best_global_step": 792,
  "best_metric": 0.7648054145516074,
  "best_model_checkpoint": "./results/checkpoint-792",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 792,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012626262626262626,
      "grad_norm": 1.192530870437622,
      "learning_rate": 4.971590909090909e-05,
      "loss": 0.6975,
      "step": 10
    },
    {
      "epoch": 0.025252525252525252,
      "grad_norm": 1.4467155933380127,
      "learning_rate": 4.940025252525253e-05,
      "loss": 0.6835,
      "step": 20
    },
    {
      "epoch": 0.03787878787878788,
      "grad_norm": 2.1208455562591553,
      "learning_rate": 4.908459595959596e-05,
      "loss": 0.6625,
      "step": 30
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 1.954585075378418,
      "learning_rate": 4.87689393939394e-05,
      "loss": 0.6237,
      "step": 40
    },
    {
      "epoch": 0.06313131313131314,
      "grad_norm": 1.3102446794509888,
      "learning_rate": 4.845328282828283e-05,
      "loss": 0.6351,
      "step": 50
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 1.0933585166931152,
      "learning_rate": 4.813762626262627e-05,
      "loss": 0.6841,
      "step": 60
    },
    {
      "epoch": 0.08838383838383838,
      "grad_norm": 1.8948724269866943,
      "learning_rate": 4.7821969696969696e-05,
      "loss": 0.706,
      "step": 70
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 1.9556242227554321,
      "learning_rate": 4.750631313131314e-05,
      "loss": 0.6185,
      "step": 80
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 1.8156793117523193,
      "learning_rate": 4.7190656565656566e-05,
      "loss": 0.6684,
      "step": 90
    },
    {
      "epoch": 0.12626262626262627,
      "grad_norm": 1.6018415689468384,
      "learning_rate": 4.6875e-05,
      "loss": 0.6589,
      "step": 100
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 1.8146073818206787,
      "learning_rate": 4.6559343434343436e-05,
      "loss": 0.64,
      "step": 110
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 1.181432843208313,
      "learning_rate": 4.624368686868687e-05,
      "loss": 0.6737,
      "step": 120
    },
    {
      "epoch": 0.16414141414141414,
      "grad_norm": 0.9676673412322998,
      "learning_rate": 4.5928030303030306e-05,
      "loss": 0.6522,
      "step": 130
    },
    {
      "epoch": 0.17676767676767677,
      "grad_norm": 1.3135879039764404,
      "learning_rate": 4.5612373737373734e-05,
      "loss": 0.6476,
      "step": 140
    },
    {
      "epoch": 0.1893939393939394,
      "grad_norm": 3.6746041774749756,
      "learning_rate": 4.5296717171717176e-05,
      "loss": 0.667,
      "step": 150
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 1.2799841165542603,
      "learning_rate": 4.498106060606061e-05,
      "loss": 0.6547,
      "step": 160
    },
    {
      "epoch": 0.21464646464646464,
      "grad_norm": 1.4554964303970337,
      "learning_rate": 4.466540404040404e-05,
      "loss": 0.6366,
      "step": 170
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 1.2581204175949097,
      "learning_rate": 4.4349747474747474e-05,
      "loss": 0.6566,
      "step": 180
    },
    {
      "epoch": 0.2398989898989899,
      "grad_norm": 1.8956793546676636,
      "learning_rate": 4.4034090909090916e-05,
      "loss": 0.6857,
      "step": 190
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 1.0984647274017334,
      "learning_rate": 4.3718434343434344e-05,
      "loss": 0.6042,
      "step": 200
    },
    {
      "epoch": 0.26515151515151514,
      "grad_norm": 1.2777488231658936,
      "learning_rate": 4.340277777777778e-05,
      "loss": 0.6266,
      "step": 210
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 1.0702110528945923,
      "learning_rate": 4.3087121212121214e-05,
      "loss": 0.6542,
      "step": 220
    },
    {
      "epoch": 0.2904040404040404,
      "grad_norm": 2.186570644378662,
      "learning_rate": 4.277146464646465e-05,
      "loss": 0.5714,
      "step": 230
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 1.6360487937927246,
      "learning_rate": 4.2455808080808084e-05,
      "loss": 0.5864,
      "step": 240
    },
    {
      "epoch": 0.31565656565656564,
      "grad_norm": 1.178390622138977,
      "learning_rate": 4.214015151515151e-05,
      "loss": 0.643,
      "step": 250
    },
    {
      "epoch": 0.3282828282828283,
      "grad_norm": 1.0458359718322754,
      "learning_rate": 4.1824494949494955e-05,
      "loss": 0.6544,
      "step": 260
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 3.2997989654541016,
      "learning_rate": 4.150883838383838e-05,
      "loss": 0.7023,
      "step": 270
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 2.303267002105713,
      "learning_rate": 4.119318181818182e-05,
      "loss": 0.6823,
      "step": 280
    },
    {
      "epoch": 0.3661616161616162,
      "grad_norm": 3.569681167602539,
      "learning_rate": 4.087752525252525e-05,
      "loss": 0.6662,
      "step": 290
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 3.0365047454833984,
      "learning_rate": 4.056186868686869e-05,
      "loss": 0.6213,
      "step": 300
    },
    {
      "epoch": 0.39141414141414144,
      "grad_norm": 1.8809446096420288,
      "learning_rate": 4.024621212121212e-05,
      "loss": 0.63,
      "step": 310
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 1.2102679014205933,
      "learning_rate": 3.993055555555556e-05,
      "loss": 0.6403,
      "step": 320
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.165285348892212,
      "learning_rate": 3.961489898989899e-05,
      "loss": 0.6383,
      "step": 330
    },
    {
      "epoch": 0.4292929292929293,
      "grad_norm": 3.2587168216705322,
      "learning_rate": 3.929924242424243e-05,
      "loss": 0.625,
      "step": 340
    },
    {
      "epoch": 0.44191919191919193,
      "grad_norm": 1.6320310831069946,
      "learning_rate": 3.8983585858585856e-05,
      "loss": 0.6263,
      "step": 350
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 1.4929893016815186,
      "learning_rate": 3.86679292929293e-05,
      "loss": 0.6449,
      "step": 360
    },
    {
      "epoch": 0.4671717171717172,
      "grad_norm": 1.3952044248580933,
      "learning_rate": 3.835227272727273e-05,
      "loss": 0.6662,
      "step": 370
    },
    {
      "epoch": 0.4797979797979798,
      "grad_norm": 2.886904716491699,
      "learning_rate": 3.803661616161616e-05,
      "loss": 0.6697,
      "step": 380
    },
    {
      "epoch": 0.49242424242424243,
      "grad_norm": 2.595059871673584,
      "learning_rate": 3.7720959595959596e-05,
      "loss": 0.5807,
      "step": 390
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 1.1865818500518799,
      "learning_rate": 3.740530303030303e-05,
      "loss": 0.6061,
      "step": 400
    },
    {
      "epoch": 0.5176767676767676,
      "grad_norm": 1.2513346672058105,
      "learning_rate": 3.7089646464646466e-05,
      "loss": 0.7201,
      "step": 410
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 5.018141269683838,
      "learning_rate": 3.67739898989899e-05,
      "loss": 0.6587,
      "step": 420
    },
    {
      "epoch": 0.5429292929292929,
      "grad_norm": 3.571751117706299,
      "learning_rate": 3.6458333333333336e-05,
      "loss": 0.606,
      "step": 430
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.747352957725525,
      "learning_rate": 3.614267676767677e-05,
      "loss": 0.5919,
      "step": 440
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 1.1746881008148193,
      "learning_rate": 3.5827020202020206e-05,
      "loss": 0.6075,
      "step": 450
    },
    {
      "epoch": 0.5808080808080808,
      "grad_norm": 1.1757769584655762,
      "learning_rate": 3.5511363636363635e-05,
      "loss": 0.5914,
      "step": 460
    },
    {
      "epoch": 0.5934343434343434,
      "grad_norm": 1.7587096691131592,
      "learning_rate": 3.5195707070707076e-05,
      "loss": 0.6311,
      "step": 470
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 2.653165578842163,
      "learning_rate": 3.4880050505050505e-05,
      "loss": 0.6268,
      "step": 480
    },
    {
      "epoch": 0.6186868686868687,
      "grad_norm": 1.6993149518966675,
      "learning_rate": 3.456439393939394e-05,
      "loss": 0.5961,
      "step": 490
    },
    {
      "epoch": 0.6313131313131313,
      "grad_norm": 3.97052264213562,
      "learning_rate": 3.4248737373737375e-05,
      "loss": 0.6923,
      "step": 500
    },
    {
      "epoch": 0.6439393939393939,
      "grad_norm": 1.2173269987106323,
      "learning_rate": 3.393308080808081e-05,
      "loss": 0.6915,
      "step": 510
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 1.773938775062561,
      "learning_rate": 3.3617424242424245e-05,
      "loss": 0.6264,
      "step": 520
    },
    {
      "epoch": 0.6691919191919192,
      "grad_norm": 2.990946054458618,
      "learning_rate": 3.330176767676767e-05,
      "loss": 0.6481,
      "step": 530
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 1.3411622047424316,
      "learning_rate": 3.2986111111111115e-05,
      "loss": 0.6329,
      "step": 540
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 1.6717828512191772,
      "learning_rate": 3.267045454545455e-05,
      "loss": 0.7245,
      "step": 550
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 2.4353830814361572,
      "learning_rate": 3.235479797979798e-05,
      "loss": 0.6221,
      "step": 560
    },
    {
      "epoch": 0.7196969696969697,
      "grad_norm": 1.531101107597351,
      "learning_rate": 3.203914141414141e-05,
      "loss": 0.6755,
      "step": 570
    },
    {
      "epoch": 0.7323232323232324,
      "grad_norm": 3.0911262035369873,
      "learning_rate": 3.1723484848484855e-05,
      "loss": 0.6169,
      "step": 580
    },
    {
      "epoch": 0.7449494949494949,
      "grad_norm": 1.546155333518982,
      "learning_rate": 3.140782828282828e-05,
      "loss": 0.63,
      "step": 590
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 1.3895807266235352,
      "learning_rate": 3.109217171717172e-05,
      "loss": 0.6009,
      "step": 600
    },
    {
      "epoch": 0.7702020202020202,
      "grad_norm": 1.1514644622802734,
      "learning_rate": 3.077651515151515e-05,
      "loss": 0.6102,
      "step": 610
    },
    {
      "epoch": 0.7828282828282829,
      "grad_norm": 2.856458902359009,
      "learning_rate": 3.0460858585858588e-05,
      "loss": 0.6227,
      "step": 620
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 1.1294395923614502,
      "learning_rate": 3.0145202020202023e-05,
      "loss": 0.6064,
      "step": 630
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 1.5512977838516235,
      "learning_rate": 2.9829545454545455e-05,
      "loss": 0.6324,
      "step": 640
    },
    {
      "epoch": 0.8207070707070707,
      "grad_norm": 2.8627119064331055,
      "learning_rate": 2.951388888888889e-05,
      "loss": 0.7207,
      "step": 650
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 3.4412591457366943,
      "learning_rate": 2.9198232323232328e-05,
      "loss": 0.6232,
      "step": 660
    },
    {
      "epoch": 0.8459595959595959,
      "grad_norm": 1.2810953855514526,
      "learning_rate": 2.888257575757576e-05,
      "loss": 0.5585,
      "step": 670
    },
    {
      "epoch": 0.8585858585858586,
      "grad_norm": 1.3478280305862427,
      "learning_rate": 2.8566919191919195e-05,
      "loss": 0.5723,
      "step": 680
    },
    {
      "epoch": 0.8712121212121212,
      "grad_norm": 3.006052017211914,
      "learning_rate": 2.8251262626262627e-05,
      "loss": 0.6095,
      "step": 690
    },
    {
      "epoch": 0.8838383838383839,
      "grad_norm": 1.3034414052963257,
      "learning_rate": 2.793560606060606e-05,
      "loss": 0.6756,
      "step": 700
    },
    {
      "epoch": 0.8964646464646465,
      "grad_norm": 1.2919070720672607,
      "learning_rate": 2.76199494949495e-05,
      "loss": 0.609,
      "step": 710
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 2.0952212810516357,
      "learning_rate": 2.7304292929292928e-05,
      "loss": 0.6813,
      "step": 720
    },
    {
      "epoch": 0.9217171717171717,
      "grad_norm": 1.4177844524383545,
      "learning_rate": 2.6988636363636367e-05,
      "loss": 0.6575,
      "step": 730
    },
    {
      "epoch": 0.9343434343434344,
      "grad_norm": 1.672208309173584,
      "learning_rate": 2.6672979797979798e-05,
      "loss": 0.6396,
      "step": 740
    },
    {
      "epoch": 0.946969696969697,
      "grad_norm": 1.719705581665039,
      "learning_rate": 2.6357323232323233e-05,
      "loss": 0.6247,
      "step": 750
    },
    {
      "epoch": 0.9595959595959596,
      "grad_norm": 1.7475638389587402,
      "learning_rate": 2.604166666666667e-05,
      "loss": 0.6355,
      "step": 760
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 2.25580096244812,
      "learning_rate": 2.57260101010101e-05,
      "loss": 0.6141,
      "step": 770
    },
    {
      "epoch": 0.9848484848484849,
      "grad_norm": 1.7572846412658691,
      "learning_rate": 2.541035353535354e-05,
      "loss": 0.6281,
      "step": 780
    },
    {
      "epoch": 0.9974747474747475,
      "grad_norm": 2.0385823249816895,
      "learning_rate": 2.5094696969696973e-05,
      "loss": 0.6167,
      "step": 790
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7648054145516074,
      "eval_f1": 0.0,
      "eval_loss": 0.5764978528022766,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 52.6459,
      "eval_samples_per_second": 56.13,
      "eval_steps_per_second": 7.028,
      "step": 792
    }
  ],
  "logging_steps": 10,
  "max_steps": 1584,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 213180525373440.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
