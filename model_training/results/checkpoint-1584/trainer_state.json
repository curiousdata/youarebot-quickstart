{
  "best_global_step": 1584,
  "best_metric": 0.8192893401015229,
  "best_model_checkpoint": "./results/checkpoint-1584",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1584,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012626262626262626,
      "grad_norm": 0.9610921144485474,
      "learning_rate": 4.971590909090909e-05,
      "loss": 0.6986,
      "step": 10
    },
    {
      "epoch": 0.025252525252525252,
      "grad_norm": 1.4382768869400024,
      "learning_rate": 4.940025252525253e-05,
      "loss": 0.6792,
      "step": 20
    },
    {
      "epoch": 0.03787878787878788,
      "grad_norm": 1.802415370941162,
      "learning_rate": 4.908459595959596e-05,
      "loss": 0.6698,
      "step": 30
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 1.7340192794799805,
      "learning_rate": 4.87689393939394e-05,
      "loss": 0.6395,
      "step": 40
    },
    {
      "epoch": 0.06313131313131314,
      "grad_norm": 1.365942120552063,
      "learning_rate": 4.845328282828283e-05,
      "loss": 0.6323,
      "step": 50
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 1.051182746887207,
      "learning_rate": 4.813762626262627e-05,
      "loss": 0.6962,
      "step": 60
    },
    {
      "epoch": 0.08838383838383838,
      "grad_norm": 1.5688961744308472,
      "learning_rate": 4.7821969696969696e-05,
      "loss": 0.7057,
      "step": 70
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 1.961564064025879,
      "learning_rate": 4.750631313131314e-05,
      "loss": 0.6269,
      "step": 80
    },
    {
      "epoch": 0.11363636363636363,
      "grad_norm": 1.5098240375518799,
      "learning_rate": 4.7190656565656566e-05,
      "loss": 0.6686,
      "step": 90
    },
    {
      "epoch": 0.12626262626262627,
      "grad_norm": 1.4512279033660889,
      "learning_rate": 4.6875e-05,
      "loss": 0.6595,
      "step": 100
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 1.598847508430481,
      "learning_rate": 4.6559343434343436e-05,
      "loss": 0.6418,
      "step": 110
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 1.1083307266235352,
      "learning_rate": 4.624368686868687e-05,
      "loss": 0.6765,
      "step": 120
    },
    {
      "epoch": 0.16414141414141414,
      "grad_norm": 0.8946175575256348,
      "learning_rate": 4.5928030303030306e-05,
      "loss": 0.6558,
      "step": 130
    },
    {
      "epoch": 0.17676767676767677,
      "grad_norm": 1.198638677597046,
      "learning_rate": 4.5612373737373734e-05,
      "loss": 0.6556,
      "step": 140
    },
    {
      "epoch": 0.1893939393939394,
      "grad_norm": 3.382347822189331,
      "learning_rate": 4.5296717171717176e-05,
      "loss": 0.6554,
      "step": 150
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 1.2579902410507202,
      "learning_rate": 4.498106060606061e-05,
      "loss": 0.6544,
      "step": 160
    },
    {
      "epoch": 0.21464646464646464,
      "grad_norm": 1.2961782217025757,
      "learning_rate": 4.466540404040404e-05,
      "loss": 0.642,
      "step": 170
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 1.2270352840423584,
      "learning_rate": 4.4349747474747474e-05,
      "loss": 0.6654,
      "step": 180
    },
    {
      "epoch": 0.2398989898989899,
      "grad_norm": 1.7567654848098755,
      "learning_rate": 4.4034090909090916e-05,
      "loss": 0.6814,
      "step": 190
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 0.9581700563430786,
      "learning_rate": 4.3718434343434344e-05,
      "loss": 0.6167,
      "step": 200
    },
    {
      "epoch": 0.26515151515151514,
      "grad_norm": 1.071961760520935,
      "learning_rate": 4.340277777777778e-05,
      "loss": 0.6351,
      "step": 210
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.9397538304328918,
      "learning_rate": 4.3087121212121214e-05,
      "loss": 0.6475,
      "step": 220
    },
    {
      "epoch": 0.2904040404040404,
      "grad_norm": 2.0223045349121094,
      "learning_rate": 4.277146464646465e-05,
      "loss": 0.58,
      "step": 230
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 1.3892736434936523,
      "learning_rate": 4.2455808080808084e-05,
      "loss": 0.5844,
      "step": 240
    },
    {
      "epoch": 0.31565656565656564,
      "grad_norm": 1.086022973060608,
      "learning_rate": 4.214015151515151e-05,
      "loss": 0.6384,
      "step": 250
    },
    {
      "epoch": 0.3282828282828283,
      "grad_norm": 0.912186324596405,
      "learning_rate": 4.1824494949494955e-05,
      "loss": 0.6421,
      "step": 260
    },
    {
      "epoch": 0.3409090909090909,
      "grad_norm": 3.194878101348877,
      "learning_rate": 4.150883838383838e-05,
      "loss": 0.6983,
      "step": 270
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 2.157341957092285,
      "learning_rate": 4.119318181818182e-05,
      "loss": 0.6808,
      "step": 280
    },
    {
      "epoch": 0.3661616161616162,
      "grad_norm": 3.158125877380371,
      "learning_rate": 4.087752525252525e-05,
      "loss": 0.6582,
      "step": 290
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 2.757783889770508,
      "learning_rate": 4.056186868686869e-05,
      "loss": 0.6301,
      "step": 300
    },
    {
      "epoch": 0.39141414141414144,
      "grad_norm": 1.8227592706680298,
      "learning_rate": 4.024621212121212e-05,
      "loss": 0.6315,
      "step": 310
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 1.078497290611267,
      "learning_rate": 3.993055555555556e-05,
      "loss": 0.6449,
      "step": 320
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.056829810142517,
      "learning_rate": 3.961489898989899e-05,
      "loss": 0.6374,
      "step": 330
    },
    {
      "epoch": 0.4292929292929293,
      "grad_norm": 3.0038695335388184,
      "learning_rate": 3.929924242424243e-05,
      "loss": 0.6205,
      "step": 340
    },
    {
      "epoch": 0.44191919191919193,
      "grad_norm": 1.4214117527008057,
      "learning_rate": 3.8983585858585856e-05,
      "loss": 0.6197,
      "step": 350
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 1.3154090642929077,
      "learning_rate": 3.86679292929293e-05,
      "loss": 0.6445,
      "step": 360
    },
    {
      "epoch": 0.4671717171717172,
      "grad_norm": 1.34840726852417,
      "learning_rate": 3.835227272727273e-05,
      "loss": 0.6534,
      "step": 370
    },
    {
      "epoch": 0.4797979797979798,
      "grad_norm": 2.4055354595184326,
      "learning_rate": 3.803661616161616e-05,
      "loss": 0.6847,
      "step": 380
    },
    {
      "epoch": 0.49242424242424243,
      "grad_norm": 2.438746690750122,
      "learning_rate": 3.7720959595959596e-05,
      "loss": 0.5747,
      "step": 390
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 1.028798222541809,
      "learning_rate": 3.740530303030303e-05,
      "loss": 0.5925,
      "step": 400
    },
    {
      "epoch": 0.5176767676767676,
      "grad_norm": 1.110426664352417,
      "learning_rate": 3.7089646464646466e-05,
      "loss": 0.7094,
      "step": 410
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 4.4045305252075195,
      "learning_rate": 3.67739898989899e-05,
      "loss": 0.6538,
      "step": 420
    },
    {
      "epoch": 0.5429292929292929,
      "grad_norm": 3.336294651031494,
      "learning_rate": 3.6458333333333336e-05,
      "loss": 0.6046,
      "step": 430
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.4930108785629272,
      "learning_rate": 3.614267676767677e-05,
      "loss": 0.5858,
      "step": 440
    },
    {
      "epoch": 0.5681818181818182,
      "grad_norm": 1.0946584939956665,
      "learning_rate": 3.5827020202020206e-05,
      "loss": 0.6097,
      "step": 450
    },
    {
      "epoch": 0.5808080808080808,
      "grad_norm": 1.0822950601577759,
      "learning_rate": 3.5511363636363635e-05,
      "loss": 0.587,
      "step": 460
    },
    {
      "epoch": 0.5934343434343434,
      "grad_norm": 1.72153902053833,
      "learning_rate": 3.5195707070707076e-05,
      "loss": 0.6255,
      "step": 470
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 2.4062020778656006,
      "learning_rate": 3.4880050505050505e-05,
      "loss": 0.6318,
      "step": 480
    },
    {
      "epoch": 0.6186868686868687,
      "grad_norm": 1.5430848598480225,
      "learning_rate": 3.456439393939394e-05,
      "loss": 0.5964,
      "step": 490
    },
    {
      "epoch": 0.6313131313131313,
      "grad_norm": 3.741588592529297,
      "learning_rate": 3.4248737373737375e-05,
      "loss": 0.694,
      "step": 500
    },
    {
      "epoch": 0.6439393939393939,
      "grad_norm": 1.1364691257476807,
      "learning_rate": 3.393308080808081e-05,
      "loss": 0.69,
      "step": 510
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 1.650371789932251,
      "learning_rate": 3.3617424242424245e-05,
      "loss": 0.6483,
      "step": 520
    },
    {
      "epoch": 0.6691919191919192,
      "grad_norm": 2.8643479347229004,
      "learning_rate": 3.330176767676767e-05,
      "loss": 0.6429,
      "step": 530
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 1.1532886028289795,
      "learning_rate": 3.2986111111111115e-05,
      "loss": 0.6313,
      "step": 540
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 1.6895354986190796,
      "learning_rate": 3.267045454545455e-05,
      "loss": 0.7141,
      "step": 550
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 2.335181951522827,
      "learning_rate": 3.235479797979798e-05,
      "loss": 0.6255,
      "step": 560
    },
    {
      "epoch": 0.7196969696969697,
      "grad_norm": 1.2466596364974976,
      "learning_rate": 3.203914141414141e-05,
      "loss": 0.6818,
      "step": 570
    },
    {
      "epoch": 0.7323232323232324,
      "grad_norm": 2.692542314529419,
      "learning_rate": 3.1723484848484855e-05,
      "loss": 0.6123,
      "step": 580
    },
    {
      "epoch": 0.7449494949494949,
      "grad_norm": 1.5404471158981323,
      "learning_rate": 3.140782828282828e-05,
      "loss": 0.6346,
      "step": 590
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 1.2440273761749268,
      "learning_rate": 3.109217171717172e-05,
      "loss": 0.5988,
      "step": 600
    },
    {
      "epoch": 0.7702020202020202,
      "grad_norm": 1.1141574382781982,
      "learning_rate": 3.077651515151515e-05,
      "loss": 0.6134,
      "step": 610
    },
    {
      "epoch": 0.7828282828282829,
      "grad_norm": 2.6813302040100098,
      "learning_rate": 3.0460858585858588e-05,
      "loss": 0.6221,
      "step": 620
    },
    {
      "epoch": 0.7954545454545454,
      "grad_norm": 1.0300333499908447,
      "learning_rate": 3.0145202020202023e-05,
      "loss": 0.6141,
      "step": 630
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 1.3891613483428955,
      "learning_rate": 2.9829545454545455e-05,
      "loss": 0.6394,
      "step": 640
    },
    {
      "epoch": 0.8207070707070707,
      "grad_norm": 2.5616018772125244,
      "learning_rate": 2.951388888888889e-05,
      "loss": 0.7072,
      "step": 650
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 3.258668899536133,
      "learning_rate": 2.9198232323232328e-05,
      "loss": 0.6251,
      "step": 660
    },
    {
      "epoch": 0.8459595959595959,
      "grad_norm": 1.306289553642273,
      "learning_rate": 2.888257575757576e-05,
      "loss": 0.5602,
      "step": 670
    },
    {
      "epoch": 0.8585858585858586,
      "grad_norm": 1.3442095518112183,
      "learning_rate": 2.8566919191919195e-05,
      "loss": 0.5701,
      "step": 680
    },
    {
      "epoch": 0.8712121212121212,
      "grad_norm": 2.7435951232910156,
      "learning_rate": 2.8251262626262627e-05,
      "loss": 0.6159,
      "step": 690
    },
    {
      "epoch": 0.8838383838383839,
      "grad_norm": 1.155961036682129,
      "learning_rate": 2.793560606060606e-05,
      "loss": 0.6861,
      "step": 700
    },
    {
      "epoch": 0.8964646464646465,
      "grad_norm": 1.274552822113037,
      "learning_rate": 2.76199494949495e-05,
      "loss": 0.6021,
      "step": 710
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 1.763421654701233,
      "learning_rate": 2.7304292929292928e-05,
      "loss": 0.6759,
      "step": 720
    },
    {
      "epoch": 0.9217171717171717,
      "grad_norm": 1.336573839187622,
      "learning_rate": 2.6988636363636367e-05,
      "loss": 0.6608,
      "step": 730
    },
    {
      "epoch": 0.9343434343434344,
      "grad_norm": 1.460571050643921,
      "learning_rate": 2.6672979797979798e-05,
      "loss": 0.6362,
      "step": 740
    },
    {
      "epoch": 0.946969696969697,
      "grad_norm": 1.673858404159546,
      "learning_rate": 2.6357323232323233e-05,
      "loss": 0.6235,
      "step": 750
    },
    {
      "epoch": 0.9595959595959596,
      "grad_norm": 1.5557290315628052,
      "learning_rate": 2.604166666666667e-05,
      "loss": 0.6475,
      "step": 760
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 2.247614860534668,
      "learning_rate": 2.57260101010101e-05,
      "loss": 0.6104,
      "step": 770
    },
    {
      "epoch": 0.9848484848484849,
      "grad_norm": 1.5978529453277588,
      "learning_rate": 2.541035353535354e-05,
      "loss": 0.6265,
      "step": 780
    },
    {
      "epoch": 0.9974747474747475,
      "grad_norm": 1.7853052616119385,
      "learning_rate": 2.5094696969696973e-05,
      "loss": 0.6081,
      "step": 790
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7563451776649747,
      "eval_f1": 0.0,
      "eval_loss": 0.5639052391052246,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 53.0343,
      "eval_samples_per_second": 55.719,
      "eval_steps_per_second": 6.977,
      "step": 792
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 1.1568729877471924,
      "learning_rate": 2.4779040404040405e-05,
      "loss": 0.6537,
      "step": 800
    },
    {
      "epoch": 1.0227272727272727,
      "grad_norm": 2.298459768295288,
      "learning_rate": 2.4463383838383837e-05,
      "loss": 0.5902,
      "step": 810
    },
    {
      "epoch": 1.0353535353535352,
      "grad_norm": 4.635798454284668,
      "learning_rate": 2.4147727272727275e-05,
      "loss": 0.6193,
      "step": 820
    },
    {
      "epoch": 1.047979797979798,
      "grad_norm": 1.3049159049987793,
      "learning_rate": 2.3832070707070707e-05,
      "loss": 0.675,
      "step": 830
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 2.256103754043579,
      "learning_rate": 2.3516414141414142e-05,
      "loss": 0.5677,
      "step": 840
    },
    {
      "epoch": 1.0732323232323233,
      "grad_norm": 1.7694158554077148,
      "learning_rate": 2.3200757575757577e-05,
      "loss": 0.5863,
      "step": 850
    },
    {
      "epoch": 1.0858585858585859,
      "grad_norm": 1.2499287128448486,
      "learning_rate": 2.2885101010101012e-05,
      "loss": 0.6392,
      "step": 860
    },
    {
      "epoch": 1.0984848484848484,
      "grad_norm": 1.9221171140670776,
      "learning_rate": 2.2569444444444447e-05,
      "loss": 0.6102,
      "step": 870
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 1.1358286142349243,
      "learning_rate": 2.225378787878788e-05,
      "loss": 0.62,
      "step": 880
    },
    {
      "epoch": 1.1237373737373737,
      "grad_norm": 1.2915489673614502,
      "learning_rate": 2.1938131313131313e-05,
      "loss": 0.5437,
      "step": 890
    },
    {
      "epoch": 1.1363636363636362,
      "grad_norm": 2.1660585403442383,
      "learning_rate": 2.162247474747475e-05,
      "loss": 0.6759,
      "step": 900
    },
    {
      "epoch": 1.148989898989899,
      "grad_norm": 2.3862698078155518,
      "learning_rate": 2.1306818181818183e-05,
      "loss": 0.6733,
      "step": 910
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 2.573587417602539,
      "learning_rate": 2.099116161616162e-05,
      "loss": 0.6038,
      "step": 920
    },
    {
      "epoch": 1.1742424242424243,
      "grad_norm": 3.102386951446533,
      "learning_rate": 2.067550505050505e-05,
      "loss": 0.6498,
      "step": 930
    },
    {
      "epoch": 1.1868686868686869,
      "grad_norm": 1.6600186824798584,
      "learning_rate": 2.035984848484849e-05,
      "loss": 0.6958,
      "step": 940
    },
    {
      "epoch": 1.1994949494949494,
      "grad_norm": 3.5606067180633545,
      "learning_rate": 2.004419191919192e-05,
      "loss": 0.6587,
      "step": 950
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 2.160001754760742,
      "learning_rate": 1.9728535353535355e-05,
      "loss": 0.6027,
      "step": 960
    },
    {
      "epoch": 1.2247474747474747,
      "grad_norm": 3.503499984741211,
      "learning_rate": 1.9412878787878787e-05,
      "loss": 0.5746,
      "step": 970
    },
    {
      "epoch": 1.2373737373737375,
      "grad_norm": 1.8462430238723755,
      "learning_rate": 1.9097222222222222e-05,
      "loss": 0.6228,
      "step": 980
    },
    {
      "epoch": 1.25,
      "grad_norm": 3.024866819381714,
      "learning_rate": 1.8781565656565657e-05,
      "loss": 0.5983,
      "step": 990
    },
    {
      "epoch": 1.2626262626262625,
      "grad_norm": 1.3887194395065308,
      "learning_rate": 1.8465909090909092e-05,
      "loss": 0.643,
      "step": 1000
    },
    {
      "epoch": 1.2752525252525253,
      "grad_norm": 1.5166144371032715,
      "learning_rate": 1.8150252525252527e-05,
      "loss": 0.5866,
      "step": 1010
    },
    {
      "epoch": 1.2878787878787878,
      "grad_norm": 1.3894283771514893,
      "learning_rate": 1.783459595959596e-05,
      "loss": 0.6369,
      "step": 1020
    },
    {
      "epoch": 1.3005050505050506,
      "grad_norm": 1.6160780191421509,
      "learning_rate": 1.7518939393939397e-05,
      "loss": 0.6579,
      "step": 1030
    },
    {
      "epoch": 1.3131313131313131,
      "grad_norm": 1.2226691246032715,
      "learning_rate": 1.720328282828283e-05,
      "loss": 0.6297,
      "step": 1040
    },
    {
      "epoch": 1.3257575757575757,
      "grad_norm": 1.3185228109359741,
      "learning_rate": 1.6887626262626264e-05,
      "loss": 0.6147,
      "step": 1050
    },
    {
      "epoch": 1.3383838383838385,
      "grad_norm": 2.289782762527466,
      "learning_rate": 1.6571969696969695e-05,
      "loss": 0.6419,
      "step": 1060
    },
    {
      "epoch": 1.351010101010101,
      "grad_norm": 1.6004892587661743,
      "learning_rate": 1.6256313131313134e-05,
      "loss": 0.604,
      "step": 1070
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 1.3529717922210693,
      "learning_rate": 1.594065656565657e-05,
      "loss": 0.5931,
      "step": 1080
    },
    {
      "epoch": 1.3762626262626263,
      "grad_norm": 2.3084442615509033,
      "learning_rate": 1.5625e-05,
      "loss": 0.7088,
      "step": 1090
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 1.5813995599746704,
      "learning_rate": 1.5309343434343435e-05,
      "loss": 0.6258,
      "step": 1100
    },
    {
      "epoch": 1.4015151515151514,
      "grad_norm": 1.1827011108398438,
      "learning_rate": 1.4993686868686869e-05,
      "loss": 0.5676,
      "step": 1110
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 1.750322699546814,
      "learning_rate": 1.4678030303030305e-05,
      "loss": 0.6134,
      "step": 1120
    },
    {
      "epoch": 1.4267676767676767,
      "grad_norm": 4.646817684173584,
      "learning_rate": 1.4362373737373739e-05,
      "loss": 0.5826,
      "step": 1130
    },
    {
      "epoch": 1.4393939393939394,
      "grad_norm": 1.5242586135864258,
      "learning_rate": 1.4046717171717172e-05,
      "loss": 0.5904,
      "step": 1140
    },
    {
      "epoch": 1.452020202020202,
      "grad_norm": 1.1643803119659424,
      "learning_rate": 1.3731060606060605e-05,
      "loss": 0.6205,
      "step": 1150
    },
    {
      "epoch": 1.4646464646464645,
      "grad_norm": 1.1434961557388306,
      "learning_rate": 1.3415404040404042e-05,
      "loss": 0.6267,
      "step": 1160
    },
    {
      "epoch": 1.4772727272727273,
      "grad_norm": 2.1994831562042236,
      "learning_rate": 1.3099747474747475e-05,
      "loss": 0.6172,
      "step": 1170
    },
    {
      "epoch": 1.4898989898989898,
      "grad_norm": 1.2807843685150146,
      "learning_rate": 1.2784090909090909e-05,
      "loss": 0.6238,
      "step": 1180
    },
    {
      "epoch": 1.5025252525252526,
      "grad_norm": 2.070387840270996,
      "learning_rate": 1.2468434343434344e-05,
      "loss": 0.6564,
      "step": 1190
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 1.2953189611434937,
      "learning_rate": 1.2152777777777779e-05,
      "loss": 0.6457,
      "step": 1200
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 1.8293650150299072,
      "learning_rate": 1.1837121212121214e-05,
      "loss": 0.5546,
      "step": 1210
    },
    {
      "epoch": 1.5404040404040404,
      "grad_norm": 1.3600366115570068,
      "learning_rate": 1.1521464646464647e-05,
      "loss": 0.6732,
      "step": 1220
    },
    {
      "epoch": 1.553030303030303,
      "grad_norm": 1.7921472787857056,
      "learning_rate": 1.1205808080808082e-05,
      "loss": 0.5954,
      "step": 1230
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 2.0108730792999268,
      "learning_rate": 1.0890151515151515e-05,
      "loss": 0.5788,
      "step": 1240
    },
    {
      "epoch": 1.5782828282828283,
      "grad_norm": 1.2245798110961914,
      "learning_rate": 1.0574494949494949e-05,
      "loss": 0.6279,
      "step": 1250
    },
    {
      "epoch": 1.5909090909090908,
      "grad_norm": 2.325313091278076,
      "learning_rate": 1.0258838383838384e-05,
      "loss": 0.7487,
      "step": 1260
    },
    {
      "epoch": 1.6035353535353534,
      "grad_norm": 1.337496042251587,
      "learning_rate": 9.943181818181819e-06,
      "loss": 0.6622,
      "step": 1270
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 1.2303428649902344,
      "learning_rate": 9.627525252525254e-06,
      "loss": 0.5867,
      "step": 1280
    },
    {
      "epoch": 1.628787878787879,
      "grad_norm": 1.2476080656051636,
      "learning_rate": 9.311868686868687e-06,
      "loss": 0.5812,
      "step": 1290
    },
    {
      "epoch": 1.6414141414141414,
      "grad_norm": 1.871458888053894,
      "learning_rate": 8.996212121212122e-06,
      "loss": 0.6246,
      "step": 1300
    },
    {
      "epoch": 1.654040404040404,
      "grad_norm": 1.4755003452301025,
      "learning_rate": 8.680555555555556e-06,
      "loss": 0.6187,
      "step": 1310
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.5337868928909302,
      "learning_rate": 8.36489898989899e-06,
      "loss": 0.5815,
      "step": 1320
    },
    {
      "epoch": 1.6792929292929293,
      "grad_norm": 1.5499035120010376,
      "learning_rate": 8.049242424242424e-06,
      "loss": 0.5477,
      "step": 1330
    },
    {
      "epoch": 1.691919191919192,
      "grad_norm": 3.378237009048462,
      "learning_rate": 7.733585858585859e-06,
      "loss": 0.6049,
      "step": 1340
    },
    {
      "epoch": 1.7045454545454546,
      "grad_norm": 3.976372718811035,
      "learning_rate": 7.417929292929293e-06,
      "loss": 0.6117,
      "step": 1350
    },
    {
      "epoch": 1.7171717171717171,
      "grad_norm": 2.050556182861328,
      "learning_rate": 7.102272727272728e-06,
      "loss": 0.6544,
      "step": 1360
    },
    {
      "epoch": 1.7297979797979797,
      "grad_norm": 1.6262410879135132,
      "learning_rate": 6.786616161616162e-06,
      "loss": 0.5958,
      "step": 1370
    },
    {
      "epoch": 1.7424242424242424,
      "grad_norm": 1.7494102716445923,
      "learning_rate": 6.470959595959596e-06,
      "loss": 0.666,
      "step": 1380
    },
    {
      "epoch": 1.7550505050505052,
      "grad_norm": 2.4038491249084473,
      "learning_rate": 6.155303030303031e-06,
      "loss": 0.6057,
      "step": 1390
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 1.5319701433181763,
      "learning_rate": 5.839646464646465e-06,
      "loss": 0.6297,
      "step": 1400
    },
    {
      "epoch": 1.7803030303030303,
      "grad_norm": 1.6038199663162231,
      "learning_rate": 5.5239898989899e-06,
      "loss": 0.6408,
      "step": 1410
    },
    {
      "epoch": 1.7929292929292928,
      "grad_norm": 1.9749505519866943,
      "learning_rate": 5.208333333333334e-06,
      "loss": 0.6234,
      "step": 1420
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 1.5024255514144897,
      "learning_rate": 4.892676767676768e-06,
      "loss": 0.6623,
      "step": 1430
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 1.771809697151184,
      "learning_rate": 4.577020202020202e-06,
      "loss": 0.6535,
      "step": 1440
    },
    {
      "epoch": 1.8308080808080809,
      "grad_norm": 2.9769182205200195,
      "learning_rate": 4.2613636363636365e-06,
      "loss": 0.6312,
      "step": 1450
    },
    {
      "epoch": 1.8434343434343434,
      "grad_norm": 1.4797745943069458,
      "learning_rate": 3.945707070707071e-06,
      "loss": 0.6298,
      "step": 1460
    },
    {
      "epoch": 1.856060606060606,
      "grad_norm": 1.2174779176712036,
      "learning_rate": 3.630050505050505e-06,
      "loss": 0.5655,
      "step": 1470
    },
    {
      "epoch": 1.8686868686868687,
      "grad_norm": 1.414589285850525,
      "learning_rate": 3.3143939393939395e-06,
      "loss": 0.6933,
      "step": 1480
    },
    {
      "epoch": 1.8813131313131313,
      "grad_norm": 3.2236831188201904,
      "learning_rate": 2.9987373737373737e-06,
      "loss": 0.5946,
      "step": 1490
    },
    {
      "epoch": 1.893939393939394,
      "grad_norm": 2.2113869190216064,
      "learning_rate": 2.6830808080808082e-06,
      "loss": 0.6322,
      "step": 1500
    },
    {
      "epoch": 1.9065656565656566,
      "grad_norm": 1.6063861846923828,
      "learning_rate": 2.3674242424242424e-06,
      "loss": 0.6466,
      "step": 1510
    },
    {
      "epoch": 1.9191919191919191,
      "grad_norm": 4.501046180725098,
      "learning_rate": 2.051767676767677e-06,
      "loss": 0.642,
      "step": 1520
    },
    {
      "epoch": 1.9318181818181817,
      "grad_norm": 2.90720272064209,
      "learning_rate": 1.7361111111111112e-06,
      "loss": 0.6407,
      "step": 1530
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 1.471799373626709,
      "learning_rate": 1.4204545454545456e-06,
      "loss": 0.655,
      "step": 1540
    },
    {
      "epoch": 1.9570707070707072,
      "grad_norm": 1.6235209703445435,
      "learning_rate": 1.1047979797979798e-06,
      "loss": 0.6183,
      "step": 1550
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 1.1498076915740967,
      "learning_rate": 7.891414141414142e-07,
      "loss": 0.5927,
      "step": 1560
    },
    {
      "epoch": 1.9823232323232323,
      "grad_norm": 2.712388515472412,
      "learning_rate": 4.7348484848484853e-07,
      "loss": 0.667,
      "step": 1570
    },
    {
      "epoch": 1.9949494949494948,
      "grad_norm": 1.7301552295684814,
      "learning_rate": 1.5782828282828283e-07,
      "loss": 0.6041,
      "step": 1580
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8192893401015229,
      "eval_f1": 0.0,
      "eval_loss": 0.4934620261192322,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 58.9846,
      "eval_samples_per_second": 50.098,
      "eval_steps_per_second": 6.273,
      "step": 1584
    }
  ],
  "logging_steps": 10,
  "max_steps": 1584,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 426361050746880.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
